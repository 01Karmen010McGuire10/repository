---
title: Learning Linked Data Topics with comments
date: '2017-09-01T16:21:09+01:00'
description: 
draft: false
creators: []
contributors: []
publisher: 
tags: []
aliases:
- "/archive/mediawiki_wiki/Learning_Linked_Data_Topics_with_comments.html"
---

 **This is an archived MediaWiki page.**  
This page was last modified on 14 Aug 2012, at 05:43.  
This page has been accessed 207 times.

<table id="toc" class="toc">
  <tr>
    <td>
      <div id="toctitle">
        <h2>Contents</h2>
      </div>
      <ul>
        <li class="toclevel-1 tocsection-1"><a href="#This_document_in_context_.28March_2012.29"><span class="tocnumber">1</span> <span class="toctext">This document in context (March 2012)</span></a></li>
        <li class="toclevel-1 tocsection-2">
          <a href="#Inventory_of_Learning_Topics"><span class="tocnumber">2</span> <span class="toctext">Inventory of Learning Topics</span></a>
          <ul>
            <li class="toclevel-2 tocsection-3"><a href="#Understanding_Linked_Data"><span class="tocnumber">2.1</span> <span class="toctext">Understanding Linked Data</span></a></li>
            <li class="toclevel-2 tocsection-4"><a href="#Searching_and_Querying_Linked_Data"><span class="tocnumber">2.2</span> <span class="toctext">Searching and Querying Linked Data</span></a></li>
            <li class="toclevel-2 tocsection-5"><a href="#Creating_and_manipulating_RDF_data"><span class="tocnumber">2.3</span> <span class="toctext">Creating and manipulating RDF data</span></a></li>
            <li class="toclevel-2 tocsection-6"><a href="#Visualization"><span class="tocnumber">2.4</span> <span class="toctext">Visualization</span></a></li>
            <li class="toclevel-2 tocsection-7"><a href="#Implementing_a_Linked_Data_application"><span class="tocnumber">2.5</span> <span class="toctext">Implementing a Linked Data application</span></a></li>
            <li class="toclevel-2 tocsection-8"><a href="#Use_Cases"><span class="tocnumber">2.6</span> <span class="toctext">Use Cases</span></a></li>
            <li class="toclevel-2 tocsection-9"><a href="#Glossary"><span class="tocnumber">2.7</span> <span class="toctext">Glossary</span></a></li>
          </ul>
        </li>
      </ul>
    </td>
  </tr>
</table>

#### This document in context (March 2012) 
<dl>
<dt><a href="/mediawiki_wiki/Learning_Linked_Data_Topics.md" class="external text" rel="nofollow">Inventory of Learning Topics</a></dt>
<dd> <i><b>THIS DOCUMENT -- For internal review 13 March through 2 April, after which the text will be posted for wider review on a UW blog. Tom will fill in the @@@ placeholders with short definitions and explanatory text.</b></i>
</dd>
<dt><a href="/mediawiki_wiki/Learning_Linked_Data_Workshop_Minutes.md" class="external text" rel="nofollow">Seattle Workshop Minutes</a></dt>
<dd> Three-page summary of key discussion points and actions.
</dd>
<dt><a href="/mediawiki_wiki/Learning_Linked_Data_Workshop_Notes.md" class="external text" rel="nofollow">Seattle Workshop Notes</a></dt>
<dd> Forty-page edited transcript of workshop discussion as a reference on points of detail.
</dd>
<dt><a href="/mediawiki_wiki/Learning_Linked_Data.md" class="external text" rel="nofollow">Learning Linked Data Project</a></dt>
<dd> Main wiki page for the project, with links to the project proposal, timeline, list of participants, mailing-list archive, and all wiki drafts produced to date.
</dd>
</dl>

# **Inventory of Learning Topics** 

This document presents a draft inventory of “learning topics” to be covered by learners who want to understand, process, and create Linked Data. This draft is the product of a face-to-face workshop held in February 2012 by the Learning Linked Data Project. The [Learning Linked Data Project](http://lld.ischool.uw.edu/wp/learning/about/), funded with a one-year planning grant from the [Institute of Museum and Library Services (IMLS)](http://www.imls.gov/news/national_leadership_grant_announcement.aspx#WA), relates learning topics to various types of software utilities needed to support teaching or self-instruction, in both university and professional training contexts.

- Understanding Linked Data
- Searching and querying Linked Data
- Creating and manipulating RDF
- Visualization
- Implementing a Linked Data application
- Use cases for Linked Data instruction

The project partners plan to propose a follow-on project in 2013 for integrating the tools, thus identified, into a coherent platform.

Inasmuch as Linked Data is based on data structures of a “linguistic” nature, the guiding metaphor for the Learning Linked Data Project is that of a “language lab.” Specifically, the project aims at outlining only how the language lab should be equipped, e.g., what tools are needed for mastering the learning topics. How exactly Linked Data should be taught — the design of curricula or the sequence and selection of course materials for various audiences — is largely out of scope for the project in the same way that the pedagogical approaches or learning outcomes expected for particular courses in French or Chinese are largely beyond the scope of designing a language lab. To use a cooking metaphor, the project aims at outfitting a kitchen with utensils usable for preparing a wide range of meals.

While analogies to natural language or to cooking may help learners approach the material, different analogies will work for different audiences. As a matter of principle, the project decided to use the native terminology of RDF in outlining the tool platform — however foreign that terminology may be to many of the intended users — and leave it to instructors to bridge any conceptual gaps. The working definition of Linked Data used by the project (and repeated below), along with the working definitions of other key concepts, may be found in a separate Glossary.

This draft has been posted on the Web prior to a public comment period, which will run through June 30, 2012. Please note that until the blog is opened to comment, this document may change. The public is invited to comment on the concept of a tool platform for learning Linked Data or on the details of this outline. The project envisions the platform as a basis for the development of course modules by people involved in both formal and informal teaching and learning environments, so comments about how such a platform would be useful in particular environments would be especially welcome.

- [Jane Greenberg] would it help/be relevant to add the phrase “and a suite of applications” or means for integrating LD applications? e.g., “software platform . . . for learning to interpret and create .. .” It might be helpful to paraphrase what is mean by a software platform for the non-tech folks.
- [Jane Greenberg] I want to punch out/emphasize the “Inventory of Learning Topics.” I believe this is a major outcome of the LLD, and think perhaps to even say, “A chief outcome of this work is an Inventory of Learning Topics.” This inventory, draft or not, is a major accomplishment.
- [Drs. Sam Oh, Myongho Yi, Eun Park and Mr. Seunghun Kim]:
  - Provides a menu that shows latest news about link data
  - I think the most important part is to identify a set of requirements for a technical platform. In p. 6, the following phrase “based on an analysis of teaching needs on metadata design…” is not clear to me. I am not sure what approaches will be used to identify the technological platform.
  - When creating a technological platform for teaching Semantic Web design skills, I think keeping the balance of a technological perspective and a pedagogical perspective along with balancing the voices of participants is very necessary. Both teachers (professors or educators) and practitioners who adopt and apply the application profile to practice should be included.
  - The current linked data open sources are too hard to use. Migration process from DBMS to RDF is a good example. We need a tool that transforms to RDF while creating relationships among columns within a table. When we save an RDF, the type of operating system such as Virtuoso or D2Rserver makes a difference. Therefore, we suggest using cloud computing environment. Cloud computing creates a space to store and uses DB dump to transform to RDF. Currently, the open source tools we have now are too complicated to store and transfer. Therefore, Cloud computing environment provide solution for these issues.
- [Georg Hohmann]: You should take a look at the “WissKI” project/system, which deals with all these topics and bears some innovative approaches: [http://wiss-ki.eu](http://wiss-ki.eu) Since it is completely build upon semantic web technologies, it provides not only interfaces for “searching and querying Linked Data” and “creating and manipulating RDF” but also a module for the “visualization” of RDF data. Therefore it can be said that WissKI is a “Linked Data application”. It is available as open source ( [https://github.com/WissKI](https://github.com/WissKI)).
- [Richard Urban]: I will be offering a week-long session on Linked Data at the 2013 Digital Humanities Winter Institute ( [http://www.cmlt.umd.edu/](http://www.cmlt.umd.edu/)). I am interested in learning how to contribute to the Learning Linked Data initiative and/or pilot any tools/modules that become available in the next few months.
- [Steven Miller]: I’m teaching a new course for our MLIS program on “RDF, Ontologies, and the Semantic Web.” I am teaching the conceptual aspects, using a textbook (Antoniou & van Harmelen), various documents and reports (such as LLD), looking at examples, and using Protege for creating ontologies. But I have no IT background, and my class is aimed at students with no IT or computer science backgrounds. It would like to incorporate more “active” tools for showing how to query the linked data we create and retrieve results, preferably through a “user-firendly” search interface rather than writing SPARQL queries. Also tools for creating and manipulating RDF, visualization, use cases, etc. Really everything listed on the page [http://lld.ischool.uw.edu/wp/](http://lld.ischool.uw.edu/wp/) . I don’t have any specific comments other than that any tools that can be shared and that are fairly usable without heavy IT knowledge would greatly enhance my teaching of linked data!

### Understanding Linked Data 

Linked Data is data that fits into a “cloud” of diverse data sources — whether those sources are published world-readably on the Web (Linked Open Data) or behind a corporate or institutional firewall (Linked Enterprise Data). For the purposes of the Learning Linked Data Project, Linked Data is data published in a form compatible with the Resource Description Framework (RDF) — a Semantic Web standard of the World Wide Web Consortium (W3C). Inasmuch as RDF is a language designed for processing by machines, learners must acquire competence in the use of software tools for ingesting, visualizing, transforming, and interpreting its URI-based statements. Prerequisite to using any sort of tool, however, a learner must grasp underlying concepts such as:

- The rationale for, and application of, Linked Data
  - [Jane Greenberg]: Would be good to punch out what is meant by each of these… There is a link for data, but basic definitions or examples for what is a property, class, and instance would be good. These concepts might also be useful in the glossary.
- The elements of RDF Data: properties, classes, instances
- The node-arc model of RDF Graphs, and Named Graphs
- Reading and understanding RDF Triples (Also Merging triples to create graphs? Was in original wiki list, not in WordPress content -- DWT.)
  - [Jane Greenberg]: I like that “graphs” comes before triples. I think it helps conceptually. Is there a plan to provide a visual? Can/should this link to visualization?
  - [[Ying Ding course](http://info.slis.indiana.edu/~dingying/S636Fall2011.html) ]: Precursors to RDF -- XML Schema, with validators ( [http://www.w3.org/2001/03/webdata/xsv](http://www.w3.org/2001/03/webdata/xsv), XML Spy, Oxygen), datatypes & restrictions
  - [[Ying Ding course](http://info.slis.indiana.edu/~dingying/S636Fall2011.html) ]: Precursors to RDF -- XPATH & XQUERY ("FLOWR expressions" – for, let, orderBy, where, return, [Saxon](http://www.saxonica.com))
  - [[Ying Ding course](http://info.slis.indiana.edu/~dingying/S636Fall2011.html) ]: Ontology: classes, properties, individuals, axioms: OWL
- The use of Uniform Resource Identifiers (URIs) as globally unique identifiers
- How data is linked using URIs
- The Open World Assumption versus Closed World Assumption
- How triples are merged to create new RDF Graphs
- Principles of inferencing (reasoning) – see Ontologies
- Looking up RDF Vocabularies (“following one’s nose”)
- Principles of publishing linked data as Linked Data (e.g., content negotiation)
- (Was omitted from WordPress content, included in the original list on this wiki: Domains and ranges of RDF properties -- DWT)

**Section Comments:**

- [Jane Greenberg]: This section is clear, and the presentation of RDF is nicely partitioned. I believe it would be useful to insert a sentence between current sentence 1 + 2, to note the value of linked data for “data reuse” and/or “conceptual linking” or some way that the power of linked data is touted. I believe the point of this section is to define LD in a basic/descriptive nuts-and-bolts manner, and that the praise of what LD is—is not a focus. 
- [Jane Greenberg]: [D]o you think it is useful or confusing to include LD and LOD, LED? Eva uses LD and LOD all the time here in our communications, but I’m not sure how common these acronyms are.
- [Bernhard Haslhofer]: This sounds like a very useful project. I'd be very interested what other people teach in their course when it comes to LD. Do you know if someone already captured the state of the art in teaching LD? Some pointers to course websites would be very interesting???. Here is our our [course website](http://www.infosci.cornell.edu/Courses/info4302/2011fa/index.html) (from last year).
- [Eva Méndez]: I also now a very interesting initiative called [EUCLID: Educational Curriculum for the usage of Linked Data](http://euclid-project.eu/) which is a support action funded by the EU. It is a pity that you cannot attend tomorrow's symposium here... but you can follow it online if you can: [http://klingon.uc3m.es/hive-es/wiki/index.php/Symposium-en](http://klingon.uc3m.es/hive-es/wikiSymposium-en)
- [Craig Norvell on June 12, 2012 at 4:54 pm]: Gruff – provides a visual query builder to make creating SPARQL queries less daunting. Generates SPARQL or Prolog code. [http://www.franz.com/agraph/gruff/](http://www.franz.com/agraph/gruff/)
- [Jian Qin on June 19, 2012 at 6:54 am]: The most difficult part of teaching the concepts of linked data is how to let the students understand why linked is useful. A complete example that can show from raw RDF graphs to a human readable web page would be helpful for achieving this purpose. This example must be small and easy to replicate by learners following the steps instructed even if they have no idea how the whole thing works. Being able to see what it can turn out is a great motivating factor for figuring out how it works.
- [Bernard Haslhofer on June 19, 2012 at 9:14 am]: This sounds like a very useful project. I’d be very interested what other people teach in their course when it comes to LD. Do you know if someone already captured the state of the art in teaching LD? Some pointers to course websites would be very interesting???. Here is our our course website: [http://www.infosci.cornell.edu/Courses/info4302/2011fa/index.html](http://www.infosci.cornell.edu/Courses/info4302/2011fa/index.html) (from last year).
- [lou xiuming on June 19, 2012 at 6:10 pm]: good use cases are very important, through the use cases, we can understant the application of, the creation of , and the assumption of the linked data.
- [Gregg Kellogg on June 20, 2012 at 9:43 am]: Manu Sporny just published some really interesting videos on Linked Data and JSON-LD that are entry-level and provide a good introduction. I believe he’s planning on adding to the series. Check them out here:
  - What is Linked Data? [http://www.youtube.com/watch?v=4x\_xzT5eF5Q&feature=g-all-esi](http://www.youtube.com/watch?v=4x_xzT5eF5Q&feature=g-all-esi)
  - What is JSON-LD? [http://www.youtube.com/watch?v=vioCbTo3C-4&feature=g-all-esi](http://www.youtube.com/watch?v=vioCbTo3C-4&feature=g-all-esi)
  - [Marcia Zeng reply on June 21, 2012 at 11:45 am]: Yes, these are excellent intro for beginners, (like his RDFa video we used in teaching). Thanks for sharing the new links! I also used a real good one for my class: Video: Linked Data (and the Web of Data), by DERI and the Linked Data Research Center (LiDRC), Ireland.2010. [http://www.youtube.com/watch?v=GKfJ5onP5SQ](http://www.youtube.com/watch?v=GKfJ5onP5SQ)
- [Yves Jaques on June 22, 2012 at 2:14 am]: While all the topics above are ultimately part of the whole picture, it’s important to approach LOD on a need-to-know basis. We find that most of our users are perfectly capable of creating and/or enriching linked data without even realizing they are doing it. The list above is the skeleton, but most users don’t need an X-ray. So, it would be good to have a clear understanding of the different user types and base the teaching on that.
- [Alison Hitchens on June 25, 2012 at 6:49 am]: I introduce students to Linked Data in one lesson as a part of their classification & indexing course (LCC, DDC, LCSH). I think it is important for them to think about other ways of using subject/classification data. But because they are simply being introduced to the idea in one lesson and technical knowledge in the class really varies, I agree with Jian Qin and Lou Xiuming who have asked for good use cases or examples of the end results. I don’t have time to get into all the technical aspects, just a sense of why it is useful, the basics of RDF, and URIs. The non-technical students understand best if show end result first and then work back.
- [Sue Yeon Syn on June 26, 2012 at 3:57 pm]: The project seems to be very interesting and helpful. Students find it difficult to understand why Linked Data is important/helpful. A walk-through example of how Linked Data works and a usage case type of examples would be very significant to help users understand how it works and why it is important. Also it is important to explain elements of it such as RDF (why and how it is used). Often learners find it too complicated when they realized numerous elements are used for the whole structure. It would be nice to have a small hands-on exercise type of platform where learners can access and see what they will experience in Linked Data environment.
- [Ganwei Shi on June 26, 2012 at 8:28 pm]: I’m rookie on LOD and going to learn it in future. So I need simple and easy demos for it in order to get myself in. Hope you guys help.
- [Catherine Smith on June 30, 2012 at 9:58 am]: The six areas are very helpful for thinking about how the content relates to my courses in Digital Libraries and Information Retrieval. I think the conceptual overview is at a good level and would be very useful for both courses. “Principles of inferencing (reasoning)” should not unnecessarily complicate the introduction, however, covering the difference between a formal logical ontology and little-o ontology (RDF-based) is important. As I have commented elsewhere, tying the concepts to practice in an application-focused “language-lab” seems very important.
- [Drs. Sam Oh, Myongho Yi, Eun Park and Mr. Seunghun Kim]: 
  - Need more informational or tutorial sessions focusing on URI.
  - A guide about hands-on URI creation or a URI mapping tutorial is helpful.
  - Visual representations such as an infograph on learning roadmaps on Linked data-related technologies would also be helpful.
- [Laura Smart via email]: It would be helpful if tacit background knowledge required to understand linked data was made explicit. It seems obvious that people should understand \*nix (or at least servers of whichever platform), XML, HTML, database design, etc. before tackling this ---but, well, it’s amazing how many gaps in people’s knowledge become apparent once you begin teaching them a new thing. Links to tutorials which can provide remediation may prove useful.
  - It seems like the entire thing is geared towards people who already have a highly technical bent or background. Who is really the desired audience here? IMLS is funding it, so it’s safe to presume librarians/library workers and library/i-school students. If metadata librarians are the ones who are going to apply it vs. systems programmers, there will be a \*lot\* of remediation required. 
  - And yes, demos of “killer apps” and a list of benefits AND drawbacks to LLD (nobody ever talks about the down side). Also, there isn’t any context of how this stuff can be implemented at your garden variety academic library. What’s an appropriate pilot project? How to evaluate software and staffing you’d need to do it? Management types and resistant learners/naysayers will ask about that.
  - Most of the mentions of RDF within the context of the document referred to the presentation syntax of RDF. A knowledge of the RDF \*data model\* is required and should be made explicit.

### Searching and Querying Linked Data 

Just as a language learner must learn to ask questions and converse with native speakers, learners of Linked Data must learn how to query datasets and explore their characteristics.

- Formulating structured queries, e.g., using the SPARQL query language
  - Structured Query Tool
  - Search engines (e.g., [Sindice](http://sindice.com/), [Semantic Web Search Engine](http://swse.deri.org/))
  - [Ying Ding course](http://info.slis.indiana.edu/~dingying/S636Fall2011.html): Jena (for projects): RDF API, Ontology API, ModelMaker, Jena Sparql (ARQ query engine), SPARQLer, Protege, Prolog (in supplementary readings, not in slides)
- Assessing data and checking consistency
  - Reasoners
  - RDF validators -- [http://www.w3.org/RDF/Validator/](http://www.w3.org/RDF/Validator/)
- Discovering vocabularies
  - Vocabulary discovery tools

**Section Comments**

- [Lee Fisher]: Currently working on programming Jena, with LD datasets, learning OWL and SPARQL, and Protege. I'm slowing working on a book on Apache Jena LD programming. Perhaps an 'open courseware' style community-edited LD textbook could be part of your project?
- [Sören Auer]: We are currently developing a collaborative e-learning tool, which allows among others to create rich presentations and assessments using a crowd-sourcing approach. Our current prototype is available at [http://slidewiki.aksw.org](http://slidewiki.aksw.org). In summer we want to go Beta.
- [Adrian Walker]: You may be interested in the system and examples online:
  - [Internet Business Logic](http://www.reengineeringllc.com), A Wiki and SOA Endpoint for Executable Open Vocabulary English Q/A over SQL and RDF. Shared use is free, and there are no advertisements. 
  - [http://www.reengineeringllc.com/A\_Wiki\_for\_Business\_Rules\_in\_Open\_Vocabulary\_Executable\_English.pdf](http://www.reengineeringllc.com/A_Wiki_for_Business_Rules_in_Open_Vocabulary_Executable_English.pdf)
  - [http://www.reengineeringllc.com/demo\_agents/RDFQueryLangComparison1.agent](http://www.reengineeringllc.com/demo_agents/RDFQueryLangComparison1.agent)
- [[Ying Ding Course](http://info.slis.indiana.edu/~dingying/S636Fall2011.html) ]: Chem2Bio2RDF: Semantic Systems Chemical Biology exploration tool at [http://cheminfov.informatics.indiana.edu:8080/](http://cheminfov.informatics.indiana.edu:8080/)
- [From Ying Ding by email]: I am not sure whether you are aware of [VIVO project funded by NIH](http://www.vivoweb.org/). VIVO system is an integrated semantic web portal for annotate, store, and query linked data: [http://vivosearch.org](http://vivosearch.org), [http://vivo.iu.edu](http://vivo.iu.edu), [http://vivo.cornell.edu/](http://vivo.cornell.edu/), etc.
- [HUANG Tianqing on June 11, 2012 at 5:28 pm]: Is there any vocabulary discovery tool availble on web?Would you please just recommand it here? Thank you.
- [Pierre-Antoine Champin on June 12, 2012 at 11:28 pm]: In “assessing data”, I would put validators first and reasoners second, which makes more sense both from a practical and a pedagogical point of view. In “discovering vocabularies”, I would add a link to LOV ( [http://labs.mondeca.com/dataset/lov/](http://labs.mondeca.com/dataset/lov/)).
- [Wei Fan on June 18, 2012 at 7:07 am]: 
  - In “using the SPARQL query language” SPARQL Frontend – Pubby [http://www4.wiwiss.fu-berlin.de/pubby/](http://www4.wiwiss.fu-berlin.de/pubby/)
  - In “Discovering vocabularies” (1) Linked Open Data Cloud (static and dynamic) –Interface & Interaction Level; (2) Linked Open Datasets registry services eg. Data hub ( [http://www.datahub.org](http://www.datahub.org)) which include detailed information about specific dataset. Learners could browse existed datasets by subject and search by keyword. Additionally, learner could click one circle of LOD Cloud to link to that dataset page in LOD registry services.
- [Jian Qin on June 19, 2012 at 7:33 am]: SPARQL can be intimidating for beginners. An understanding of this language (as well as the other two areas in this section) relies on learner’s prior knowledge of schema-based data structures (XML, database), which many of them may not have. It would be more learner friendly if schema-based data structures and their roles in formulating structured queries are added at the very beginning.

[Lee Fisher June 19, 2012 at 9:16 am]: Currently working on programming Jena, with LD datasets, learning OWL and SPARQL, and Protege. I’m slowing working on a book on Apache Jena LD programming. Perhaps an ‘open courseware’ style community-edited LD textbook could be part of your project?

- [Yves Jaques on June 22, 2012 at 2:16 am]: Can be useful in this area to get users thinking about the proof and trust layers of the semantic web stack.
- [Jodi Schneider on July 6, 2012 at 9:32 am]: Writing vocabularies or application profiles might also be in scope.
- [Drs. Sam Oh, Myongho Yi, Eun Park and Mr. Seunghun Kim]:
  - Provide various sample queries that shows advantages of SQARQL.
  - Providing tools and methods to communicate between SPARQL and linked data is one of the major obstacles. Learn how to use SPARQL is not the end. How SPARQL language access data and process data need to be cleared. Current linked data don’t have a SPARQL endpoint because it is not a mandatory but a recommendation. Actually, we need more sophisticated queries without the knowledge of SPARQL. Linked data is not defined properly. A lot of room for different interpretations indicates its substantial weakness.
- [Laura Smart via email]: Pretty straightforward. I vaguely think I’ve seen some SPARQL instruction which assumes a knowledge of SQL command line database querying, so I’d be wary of it. That’s pretty much the same comment as above. Jian Qin said it best in the comments.

### Creating and manipulating RDF data 

In the Resource Description Framework (RDF), “everything is data,” from descriptions of things — both of things in the world or, more specifically, of information resources, to descriptions of the Languages of Description used to describe those things (see RDF Data). Instances, attribute spaces, and value spaces are all expressed in the same RDF language. The category “creating and manipulating data,” therefore, encompasses a broad range of topics which, in other fields, might be considered quite separate from each other:

- Creating RDF Vocabularies and minting URIs for their properties and classes
  - RDF vocabulary editors
- Creating property-to-property and class-to-class links across RDF Vocabularies
  - Mapping tools
- Creating SKOS concept schemes.
  - SKOS editors
- Creating a Domain Model. Classes of things to be described in a dataset.
  - Diagramming tools (e.g., UML and mind maps)
- Creating other types of datasets.
  - Data editors (example??)
- Converting triples among RDF syntax choices (RDF/XML, Turtle, N-Triples, RDFa)
  - Triple converters (e.g., [Rapper](http://librdf.org/raptor/rapper.html))
- Generating RDF triples from the content analysis of unstructured text data
  - Triplifiers for full text (e.g., [Calais](http://www.opencalais.com/))
- Extracting RDF triples embedded in Web pages
  - [Linters](http://linter.structured-data.org/)
  - Distillers (e.g., [RDFa Distiller](http://www.w3.org/2007/08/pyRdfa/), [Microdata to RDFa](http://www.w3.org/QA/2012/03/microdata_to_rdf_distiller.html)
- Deriving RDF triples from non-RDF data
  - Triplifiers for XML (e.g., [GRDDL](http://www.w3.org/TR/grddl/))
  - Data cleaners (e.g., [Google Refine](http://code.google.com/p/google-refine/)

**Section Comments**

- [Pierre-Antoine Champin on June 13, 2012 at 1:24 am]: In addition to Calais, you can mention NERD ( [http://nerd.eurecom.fr/](http://nerd.eurecom.fr/)) as a tool to generate triples from unstructured text.
- [Wei Fan on June 18, 2012 at 7:09 am]: 
  - Describe a RDF dataset
  - VoID Vocabulary of Interlinked Datasets [http://semanticweb.org/wiki/void](http://semanticweb.org/wiki/void)
- [Jon Voss on June 18, 2012 at 8:56 am]: I imagine you’ll want to cover Schema.org in this section. And here’s a great resource of RDFizers: [http://www.w3.org/wiki/ConverterToRdf](http://www.w3.org/wiki/ConverterToRdf)
- [Jian Qin on June 19, 2012 at 7:05 am]: Much of the legacy data is stored in relational databases or other formats such as spreadsheet or CVS files. Converting such structured data yet not in RDF syntax has already been a big problem even though with tools like D2R server. A significant chunk of this section should be devoted to the techniques and tools for converting non-RDF, structured data into linked data. As Big Data hype is on the rise, we can expect a great demand for the knowledge and skills in this area.
- [lou xiuming on June 19, 2012 at 11:53 pm]: how to publish linked data on the web: [http://sites.wiwiss.fu-berlin.de/suhl/bizer/pub/LinkedDataTutorial/20070727/](http://sites.wiwiss.fu-berlin.de/suhl/bizer/pub/LinkedDataTutorial/20070727/) ,. demonstrates several mehods to publish linked data.
- [Yves Jaques on June 22, 2012 at 2:23 am]: I would definitely put a section on standards here. We find an ongoing challenge when aligning with others based on varying use of properties, minting new properties instead of selecting existing properties, etc. Just using RDF is not enough, and a section here that goes into detail on current widely used properties, how they related and why it’s good to use them would be great.
- [Jason Zou on June 28, 2012 at 8:33 pm]: Since almost all the other headings contain “Linked Data”, it seems that it is better to use “Linked Data” instead of “RDF Data”. URIs are extremely important for Linked Data. For beginners, designing URIs for their Linked Data set may be the first hurdle to face. The Linked Data Patterns ( [http://patterns.dataincubator.org/book/](http://patterns.dataincubator.org/book/)) may be a good resource to learn existing good design patterns of Linked Data.
- [Drs. Sam Oh, Myongho Yi, Eun Park and Mr. Seunghun Kim]:
  - Tutorial on how to extract or transform data from excel, CVS, or RDBMS into RDF data.
  - Google Refine is a good tool for class. You can find more tools at [http://code.google.com/p/google-refine/wiki/RelatedSoftware](http://code.google.com/p/google-refine/wiki/RelatedSoftware)
- [Laura Smart via email]: It makes more sense to me to have this section taught prior to searching & querying. How can you effectively query if you don’t know the structure and indexing of the database (er, triple store)?

### Visualization 

Visualization plays a unique role in understanding RDF Data because RDF Graphs are conceptually diagrammatic in nature. Because in RDF, “everything is data,” some of the tools usable for visualizing instance data may be used to visualize ontologies, while other tools may be used to explore the statistical, spatial, or temporal characteristics of datasets:

- Generating node-and-arc diagrams
  - [RDF validators](http://www.w3.org/2001/sw/wiki/SWValidators)
- Generating a Linked Data cloud diagram
  - LOD cloud generators (e.g., [CKAN](http://ckan.org/)
- Visually exploring statistical characteristics of large data sets
  - Statistical visualization tools (e.g., [Spotfire](http://spotfire.tibco.com/))
- Generating different visual views of data (e.g., on timelines or maps)
  - Visualization tools (e.g., [Simile](http://simile.mit.edu/))

**Section Comments**

- [John Flynn on June 11, 2012 at 3:17 pm]: VisioOwl remains the most accurate graphical representation for OWL-based ontologies. It provides a good tool for instructors to use to teach students the basic concepts of OWL classes, subclasses, properties, subproperties and the formal relationships between them. Best of luck with your classes.
- [Craig Norvell on June 12, 2012 at 4:49 pm]: Franz Inc. provides a free tool – Gruff. [http://www.franz.com/agraph/gruff/](http://www.franz.com/agraph/gruff/) Easy to use for visualizing a dataset in Graph layout, Table View, and executing a SPARQL/Prolog query.
- [Wei Fan on June 18, 2012 at 7:11 am]: Because RDF has graph theory basis, there are some JavaScript libraries to generate web-native visualization e.g., D3.js(formerly Protovis) could RDF data as Force-Directed Graph. There is a data transformation from RDF (single file or SPARQL Endpoint result) to JSON. D3 also could generate LOD cloud and circle packing. [https://github.com/mbostock/d3/wiki/Gallery](https://github.com/mbostock/d3/wiki/Gallery).
- [Silvia Mazzini on June 18, 2012 at 2:27 pm]: We propose LodLive to browse the web of data in a new and exciting way: LodLive project provides a demonstration of the use of Linked Data standard (RDF, SPARQL) to browse RDF resources. With LodLive you can browse, explore, view on Map, view photos and more, of linked open data. LodLive offer a new way of browsing the web of data using SPARQL endpoint to obtain direct and inverse relations of a single resource. You can download the source code on github [https://github.com/dvcama/LodLive](https://github.com/dvcama/LodLive)
- [Yves Jaques on June 22, 2012 at 2:35 am]: The ontological community has been guilty for many years of fetishistically pursuing node/arc, rubber-band visualizations owing to their dubious \*wow\* factor. I would suggest keeping various visualization styles on an even keel, and spend as much or more time examining how RDF data can be put into more traditional visualizations, as when you seriously dig into the data the \*cool\* animated stuff rarely holds up as a way to actually perform analysis.
- [Sue Yeon Syn on June 26, 2012 at 4:01 pm]: Visualization would be wonderful for demonstrating the structure and elements of it. But it would be simple and clear (don’t add complexity!), and also cool and pretty. I agree that having multiple view types would be helpful.
- [Drs. Sam Oh, Myongho Yi, Eun Park and Mr. Seunghun Kim]: Provide web browser-based RDF/OW visualization tool and students can check their RDF files with this visualization tool.
- [Laura Smart via email]: Again, this is lacking some context. What are some LAM applications which visualize data. Navigating through graphs is great, CKAN if fun and all, but how do you bridge it to a LAM interface layer? Any studies on usability of the visulizations in info retrieval? (I’m guessing it’s slim and a good PhD topic). Also – this is all about arc and node diagrams. What other types of visualizations of linked data are there?

### Implementing a Linked Data application 

Simply learning how to interpret and manipulate Linked Data could stop with the topics outlined above. The extent to which a language-lab-like platform for learning Linked Data should encompass tools for implementing Linked Data applications is an open question. Whether as part of a tool platform or merely as topics of study, however, the learner should acquire knowledge of the following:

- Publishing RDF-compatible data on the Web
  - Web Frameworks (e.g., [Ruby on Rails](http://rubyonrails.org/))
  - Content Management Systems (e.g., [Drupal](http://drupal.org))
- Storing RDF data
  - RDF Triple stores (e.g., [Virtuoso](http://www.w3.org/wiki/VirtuosoUniversalServer))
  - Relational databases and other RDF-compatible backend storage options
- Integrated tool platforms
  - [LOD2 stack](http://stack.lod2.eu/)

**Section Comments**

- [Gregg Kellogg]: Documentation for [Ruby adaptations of parallel projects](http://rubydoc.info/github/ruby-rdf/linkeddata/master/frames), such as RDF::RDFa, RDF::RDFXML, RDF::Turtle, and SPARQL.
  - A potentially useful [demonstration project](https://github.com/gkellogg/github-lod) (which won't work now, due to the change in the GitHub API). If the library that I use to access the GitHub API were updated, it could probably be resurrected easily enough.
  - I actually developed the GitHub-LOD demo as part of an [introductory talk](http://www.slideshare.net/gkellogg1/ruby-semweb-20111206) I gave on Ruby RDF.
  - On the RDF WG, Nick Humphrey is a contributor, and uses it for his [DBPedia-Lite project](https://github.com/njh/dbpedialite).
- [Hugh Glaser]: I do individual stores as well: [http://sameas.org/store/](http://sameas.org/store/). [Freebase](http://sameas.org/store/freebase/) and the [British Library](http://sameas.org/store/britishlibrary/) have both given me their data to put there, and quite a few libraries have cooperated and I have put the stuff in [http://sameas.org/store/kelle/](http://sameas.org/store/kelle/). I can do a store for you if that helps. Also, tell me if you want me to mirror anything that goes in your store in the main store (although there may be a lag while that happens). You need to tell me:
  - a) the name of the store and if you want to be able to manage it yourself by putting stuff in
  - b) a username
  - c) a password
  - also, if you want a nice splash screen
  - d) a description
  - e) typical URI(s)
- [Referred by Ying Ding]: [VIVOweb](http://vivoweb.org/) is a semantically aware application that connects researchers and projects via an ontology relating their topics of investigation. It employs open-source software running under Tomcat and using the Solr search engine to return results based on records uploaded to a participating repository or otherwise available in semantic-web-compliant formats. The toolset around VIVO includes:
  - [VIVO Search](http://beta.vivosearch.org/about) -- a demonstration platform returning researcher names, publications, and institutions matching a free-text query
  - [VIVO Searchlight](http://vivosearchlight.org/) -- a browser bookmarklet that executes VIVO search based on content currently displayed in the browser window
  - [Vitro](http://sourceforge.net/p/vivo/vitro/home/VITRO%20-%20Integrated%20Ontology%20Editor%20and%20Semantic%20Web%20Application/) -- a general-purpose, web-based ontology and instance editor
  - [VIVO Harvester](http://sourceforge.net/p/vivo/harvester/home/VIVO%20Harvester:%20Enabling%20ETL%20for%20the%20National%20Network%20of%20Scientists/) -- a set of tools for automated import of researcher data from institutional repositories like faculty directories
  - [Tools for Working with VIVO data](http://sourceforge.net/p/vivo/tools/home/Home/) -- includes an extension to Google Refine for VIVO data reconciliation, SPARQL query builder, and Linked Data index builder, plus modules, plug-ins, and extensions to integrate VIVO with Refworks and the Drupal and WordPress content management platforms
  - [Download of VIVO application files](http://vivoweb.org/download) and [installation instructions](http://vivoweb.org/support/user-guide/installation)
  - [VIVO ontology](http://sourceforge.net/apps/mediawiki/vivo/index.php?title=Ontology) -- documentation with link to download the OWL resource
- [Wei Fan on June 18, 2012 at 7:13 am]: 
  - Publishing RDF data
  - Could it be considered to catalog Web Frameworks by different programming platform, e.g. Python – CherryPy+RDFlib
  - Neologism Vocabulary publishing framework [http://neologism.deri.ie/](http://neologism.deri.ie/)
  - PublishMyData Service from Swirrl [http://www.wirrl.com/](http://www.wirrl.com/)
- [Cuijuan Xia on June 18, 2012 at 6:29 pm]: Hello! I‘m from Shang Hai Library, China. My colleagues and I are very interested in Linked Data and the related technologies. It’s very great that there is a project like this. What I mainly focus on currently is the consuming technologies of Linked Data, such as, how to mashup Linked Data from different datasets, how to create RDF Links automatically between them with the tools like SILK, and how to create mappings between different vocabularies with something like R2R Framework. I also want to know how the semantic search engine based on Linked Data going to work.
- [Jian Qin on June 19, 2012 at 7:18 am]: There are many ways to publish RDF data, but the application doesn’t stop at publishing and storing. A summary of types of linked data applications should be provided and each type with at least one example. Here again, I think the examples should contain all steps and codes necessary for learners to replicate and reuse. Linked data is very different from what most people with basic web skills are familiar with, which also requires the examples to be “playable” in order for learners to see what’s under the hook.
- [Cuijuan Xia on June 19, 2012 at 4:03 pm]: It’s wonderful！I love ＂Playable” examples.&nbsp;:)
- [Jason Zou on June 28, 2012 at 8:42 pm]: Joseki, TDB, and RDFLib can be used to build a playable demo system for students.
- [Gregg Kellogg on June 20, 2012 at 9:50 am]: 
  - Documentation for Ruby adaptations of parallel projects, such as RDF::RDFa, RDF::RDFXML, RDF::Turtle, and SPARQL [http://rubydoc.info/github/ruby-rdf/linkeddata/master/frames](http://rubydoc.info/github/ruby-rdf/linkeddata/master/frames)
  - A potentially useful demonstration project is: [https://github.com/gkellogg/github-lod](https://github.com/gkellogg/github-lod) (It won’t work now, due to the change in the GitHub API). If the library that I use to access the GitHub API were updated, it could probably be resurrected easily enough.
  - I actually developed the GitHub-LOD demo as part of an introductory talk I gave on Ruby RDF: [http://www.slideshare.net/gkellogg1/ruby-semweb-20111206](http://www.slideshare.net/gkellogg1/ruby-semweb-20111206)
  - On the RDF WG, Nick Humphrey is a contributor, and uses it for his DBPedia-Lite project: [https://github.com/njh/dbpedialite](https://github.com/njh/dbpedialite)
- [Yves Jaques on June 22, 2012 at 2:43 am]: I like what Cuijuan Xia says – focusing on low-hanging fruit like (s)mashups, this is exactly what we’ve been doing – exploiting vocabulary alignments to dynamically mash data. Easy to do and can offer students a compelling example of why linked data might just be worth the effort. Also like the mention of Drupal – you can really use it as an LOD front-end in which pages become concepts. Also could be good to mention the semantic mediawiki extension.
- [Catherine Smith on June 30, 2012 at 9:52 am]: For Digital Libraries and Information Retrieval courses, one or two robust, “hands-on” example applications would be very helpful – covering publishing, storage and other aspects of implementation, as suggested above. I agree with the idea of a diverse library of “playable examples.” Also, “sandbox” applications (implementation walk-throughs?) would be very useful. Students can learn a lot by experimenting and breaking things where there is no risk. It’s a good method for self-assessment on complex material. I could see various content areas pointing to hands-on examples such as a Digital Library application or a retrieval application, where the language is practiced as it is learned, as in a language-lab.
- [Drs. Sam Oh, Myongho Yi, Eun Park and Mr. Seunghun Kim]:
  - Provide tools that can be searched for and used for implementation of Linked data. Michael K. Bergman’s website ( [http://www.mkbergman.com/sweet-tools/](http://www.mkbergman.com/sweet-tools/)) is a good example.
  - Use examples to show how linked data tools can be used to implement linked data based application.
- [Laura Smart via email]: This is all about applications. What about the content of the data to be exposed? How does one acquire or create a data set? LC and OCLC are exposing data, how does it fit in to the wider picture of linked data? It’s like that old cookbook recipe on how to cook a wolf: first obtain a wolf. And again, it presumes a LOT of background knowledge and skills. Do you need to know a programming language to implement – I see mentions of Python, Ruby, etc.

### Use Cases 

In addition to comments on the Learning Inventory, the project invites contributions of use cases outlining possible instructional scenarios related to learning Linked Data concepts, technologies, and tools. Such practical applications can help to discover and prioritize tools to be highlighted in implementing a coherent package of instructional resources.

Stuart Sutton, CEO of the Dublin Core Metadata Initiative, has provided a use case for an introductory university course. Others may be posted as comments on this page.

**SCENARIO: Introductory university course in semantic metadata**

- Prerequiste knowledge and skills:

Learners should have a basic understanding of knowledge organization systems, XML, and database management. No prior knowledge of RDF or Semantic Web is assumed.

- Education or training context:

Learners should be motivated to learn the technology. Appropriate for advanced undergraduate informatics majors. Instruction -- lectures and discussion -- are totally online and asynchronous. A ten-week course, with roughly 20 hours of lectures and presentation and 70 hours of reading, assignments, and other activities.

- Student deliverables:

By the end of the course, students will produce serializations of RDF graphs in several syntaxes; design a domain model (class diagram); create RDF vocabularies and SKOS concept schemes; and produce RDF instance data for a student project.

- Expected learning outcomes:

Students should demonstrate a grasp of basic Linked Data and Semantic Web tools and concepts, including the principles and mechanisms for merging graphs. They should understand how to use RDF serialization syntaxes; manually draw RDF graphs; serialize frequently used N-ary triple patterns; "webify" existing controlled vocabularies. They should be able to explain the difference between the XML information set and the RDF abstract data model and demonstrate modeling skills in mapping between the two.

- Required use of tools:

Students should be able to use tools for graphically depicting domain models (class diagrams); for editing and validating RDF data; for transforming data among different RDF syntaxes; and for generating visual depictions of RDF graphs

[David Talley]: Another use case (in very rough draft) is based on experience engaging with object descriptions from museums during a project at the Univ. of Washington iSchool. Many museums have exposed collection metadata as Linked Data, and more of them may well be inspired to follow suit. The learning requirements for such a project are largely conjecture on my part, but maybe they’ll trigger some welcome edits based on wider experience. I’ve followed Stuart’s format.

- SCENARIO: Museum interested in exposing collection descriptions as semantic metadata
  - Prerequiste knowledge and skills: Learners should have a basic understanding of knowledge organization systems, and at least intermediate understanding of the local collection management system, possibly including any web publishing modules available. Some skills/understanding in relational databases needed?
  - Education or training context: Motivation to engage with other cultural institutions is probably a given. Individual learner would be a web developer or web producer, but instruction may have to engage with a policy-level decision maker and primary collection manager(s) with responsibility for collection record content. Instruction strategy would emphasize self-instruction, probably based on text accessed over the web, maybe some video. Maybe some trial & error on tool demo consoles. Engagement via discussion boards with other learner/practitioners? Timeline would likely depend on project implementation plans, or it could be quite indefinite as a side project emphasizing background knowledge.
  - Student deliverables: Deliverables would likely be much the same as for other use cases, but maybe emphasizing working test/demo prototypes, and ultimately working production instances, more than rigorous document artifacts. Training may need to emphasize the value of documentation, e.g., for project revisions over time.
  - Expected learning outcomes: Highly practical! RDF/XML, Turtle, other(?) encodings for sample data from actual collection metadata; working display templates that mash up the museum’s own data with relevant data from other institutions; server and software support specs and configurations, e.g. for a SPARQL endpoint. One key outcome might be knowledge of available data sources relevant to the museum’s collection. Trust relationships could be an important variable, depending on collection scope and type as well as particular application goals. A general natural history museum might happily source Linked Data from Wikipedia for a public education application, while a very specialized institution might select carefully for sources with specific expertise. Persistent publishing of application profiles — A specialized institution might encounter highly detailed needs for resource description within its domain. If appropriate vocabularies don’t yet exist, it may have to develop them, publish them in a persistent way, and provide for their maintenance over time, including other stakeholders within the domain in those processes.
  - Required use of tools: RDF editors and validators; utilities for mediating between back-end systems, likely proprietary vendor systems, and Linked Data output needs, e.g. SPARQL access; SPARQL query builders, consoles; tools for graphic modeling (e.g., class diagrams); use and adaptation of public metadata repositories to publish specialized vocabularies.

[Laura Smart via email]:

- I think this should have been the very top/first thing. It has some of the context which was missing in the other sections. I like [the museum] use case a lot. I think it’s generalizable to other similar use cases. There are lots of LAMs sitting on specialized information and your point re: vocabularies not even existing is well taken. I think it could be applied to my use case, which you can totally use [http://library.caltech.edu/laura/?tag=facnamesproject](http://library.caltech.edu/laura/?tag=facnamesproject) . Putting it into your format:
  - SCENARIO: Small academic research library wants to expose faculty names as linked data to incorporate into the institutional repository, researcher expertise/scholar profile applications (ex. VIVO, Community of Science), grant applications, publication tracking and metrics (esp. for institutional impact/sharing at the provost level).
  - PREREQUSITE KNOWLEDGE: What you said in [the museum] use case. Plus domain knowledge of the data content (i.e. authority records)
  - EDUCATION OR TRAINING CONTEXT: Practicing early & mid-career metadata librarians who didn’t get training like this in library school, lack some of the required background. A small amount of technical support from programmers is available as well as development servers for hands-on practice but no software tools for doing the work are yet installed. Training should be asynchronous, delivered to the individual desktop and consist of practical hands-on exercises which can be related to the types of work libraries will be doing in LD. The application of the learning (i.e. implementation of scenario project) would be done in collaborative team-based fashion
  - STUDENT DELIVERABLES: Working demo prototype. Plus exposing the LD for use within other LD applications (ex. VIVO?).
  - LEARNING OUTCOMES: What you said in [the museum] use case.
  - REQUIRED USE OF TOOLS: What you said in [the museum] use case. I might add testing the LD output by adding it into another tool.

[From the February 2012 workshop:]

- [Corey Harper]: Merge MARC, EAD, and Dublin Core records into "NYU Core" records. Taking five records, extracting parts of them to triples, merging those into a graph, and turning that into a record.
- [Joseph Busch]: "Companies want consultants to produce deliverables. What you're doing here is creating an environment for people to teach themselves. A corporate problem would be that they can't efficiently get the data they need because it's dispersed in all these different places. I educate them, get their data management people to stop doing 'data management,' and think about how their data should be made Linked Data ready without building another system. A lot of this is how to educate people in the real world who are IT professionals, library professionals, to think differently about problems they're already familiar with. This tool is focused on one part of that educational process."
- [Marjorie Hlava]: "My need is a little different; I have to deliver the actual product. I might start with specifications, but we have to take data, massage it, and put it into a system where it will work. A science association might have journals, conferences, committees, etc. on a topic. If someone has an article they discovered, they might want to link out to other things in different formats or venues that have to do with that data. The use case is being able to present on a website to the searcher who finds that article the upcoming conference, etc. It's a great use case for Linked Data capability."
- [Marjorie Hlava]: "In records management, I have a client that's a large insurance company, with 2 terabytes of undifferentiated data lying around like a bomb. It costs them over a million dollars a year to maintain the drive. The application there is to figure out the retention schedule for the data. We have a controlled vocabulary and a retention schedule, and you link the two and apply it in a crawling mechanism to assign retention schedules to every information object on the shared server. They have legal holds on some data that would otherwise fall under some of those schedules."

### Glossary 

**Domain Model**

In the design of metadata, as in software design, a domain model identifies a set of “things of interest.” In the context of Linked Data, a domain model enumerates the classes of things described by metadata and specifies how those classes relate to one another.

**Languages of Description**

In the broadest sense, languages of description are controlled sets of data-dictionary elements, attributes, concepts, properties, classes, or thesaurus or vocabulary terms used in library and information management practice for describing things in the world or, more specifically, for describing information resources. The Learning Linked Data Project is interested in languages of description that are natively expressed with, or have been translated into, the language of RDF.

**Linked Data**

Linked Data is data that fits into a “cloud” of diverse data sources – whether those sources are published world-readably on the Web (Linked Open Data) or behind a corporate or institutional firewall (Linked Enterprise Data). For the purposes of the Learning Linked Data Project, Linked Data is data published in a form compatible with the Resource Description Framework (RDF) model of the World Wide Web Consortium (W3C).

**Ontologies**

For the purposes of the Learning Linked Data Project, ontologies are formalized conceptualizations constructed using RDF vocabularies. Because in RDF “everything is data” (see RDF Data), ontologies are also considered a (special) form of data. While there is no theoretical limit to how simple an ontology may be, more complex ontologies are typically expressed using the Web Ontology Language OWL – an extension of the RDF language that supports richly expressive constructs of a logical nature. “Ontology engineering” – the construction of complex logical structures in support of sophisticated inferencing, the process by which new information is automatically derived from the logic of existing information – is beyond the scope of the Learning Linked Data Project.

**Open World Assumption**

The Open World Assumption – the technical mindset of Linked Data – says that in principle, there may always exist additional sources of data, somewhere in the world, to complement the data one has at hand. Its opposite, the Closed World Assumption, assumes that data sources are well-known and tightly controlled, as in a closed, stand-alone data silo. The Open World mindset is optimized for mashing up data from multiple sources and integrating new sources as they are discovered, whereas the Closed World mindset is optimized for validating data against specific patterns and integrity constraints in the manner of most traditional data applications. Understanding the applicability and interplay of the Open and Closed approaches is a key challenge not just for learners of Linked Data, but for designers of information systems in the Web age generally.

**Resource Description Framework (RDF)**

The W3C Resource Description Framework (RDF) is a generic model for expressing data – a model optimized to support the merging and aggregation of data from multiple sources. In RDF, a “resource” is any thing, physical or conceptual, that can be named using a Uniform Resource Identifier (URI). RDF conceptualizes data as webs of information nodes, or RDF Graphs. Connections between nodes are expressed using RDF’s language of RDF Triples. RDF is the foundation for a suite of related specifications referred to generically as Semantic Web standards.

**RDF Data**

For the purposes of the Learning Linked Data Project, RDF data is any information expressed in a form compatible with RDF and thus usable as Linked Data. Learners must grasp the principle that with RDF, “everything is data.” Descriptions of things – both of things in the world or, more specifically, of information resources – are considered “data,” but in Linked Data, the Languages of Description used to describe those things are themselves expressed with RDF and, as such, are also considered to be “data.”

**RDF Graphs**

Conceptually, RDF graphs are webs of resources linked by named relationships. In visual-diagrammatic terms, graphs are sets of “nodes” (circles and squares) connected using “arcs” (arrows). Each node-arc-node link of a graph corresponds to an RDF Triple. Graphs can be serialized for machine processing using one of several interchangeable concrete syntaxes. Graphs are a bit like paragraphs in natural language inasmuch they consist of multiple statements (RDF Triples), which are analogous to sentences.

**RDF Triples**

Conceptually, RDF Triples are statements about things – statements that are constructed according to the grammatical model of the Resource Description Framework. Like some simple sentences in natural language, triples each have a subject, a predicate, and an object which, taken together, express a complete thought. Subjects and objects may be instances of an RDF “classes” (somewhat analogous to natural-language nouns), while the predicate relationship between a subject and object is expressed with an RDF “property.” If properties and classes are the words of RDF language (see RDF Vocabularies), RDF Triples are its sentences.

**RDF Vocabularies**

In Semantic Web usage, RDF vocabularies are sets of “properties” and “classes” used for describing things. Learners may approach Linked Data with other types of “vocabulary” in mind, such as thesauri, which are typically used as value vocabularies. Some such value vocabularies may be expressed in RDF as RDF vocabularies, while others may be expressed as other types of RDF Data, such as SKOS Concept Schemes. If properties and classes are the words of the RDF language, vocabularies provide the RDF language with its dictionaries.

**Semantic Web**

Semantic Web is a vision first introduced in the late 1990s by Tim Berners-Lee, according to which the distributed and globally accessible infrastructure of the World Wide Web can be leveraged to link not just documents (Web pages), but sources of structured data. In order to realize this vision, the World Wide Web Consortium (W3C) developed Semantic Web standards such as Resource Description Framework (RDF), first published as a W3C Recommendation in 1999. Starting in 2006, the movement to publish data freely on the Web using Semantic Web standards was popularized under the banner of Linked Data.

**SKOS Concept Schemes**

SKOS concept schemes are sets of related concepts – subject headings, keyword tags, thesaurus entries, or the like – expressed in RDF using Simple Knowledge Organization System (SKOS), a data model for hierarchies of broader, narrower, and related concepts. While SKOS concept schemes may resemble formal class-based Ontologies, they do not consist of “classes” that can be leveraged for advanced inferencing. Rather, concepts are modeled as “instances” that can be leveraged for the ontologically less sophisticated, though eminently useful, purposes of broadening or narrowing the scope of searches. Confusingly for people coming from a library or information management background, SKOS concept schemes are not considered to be RDF vocabularies per se, but RDF Data in a more generic sense.

**Uniform Resource Identifiers (URIs)**

Uniform Resources Identifiers, typically constructed using the http:// syntax of Web addresses, are used to name any thing of interest (“resource”), whether physical or conceptual, accessible on the Web or not.

**Section Comments**

- [Laura Akerman on June 17, 2012]: I notice you don’t make distinction between “RDF” and “RDFS” (RDF Schema) in your RDF Vocabularies section. But they are separately documented, have different namespaces, and this is an area where people new to RDF can get confused – and since RDFS is titled “RDF Vocabulary Description Language”, shouldn’t you mention it in the glossary?
  - [Tom Baker on July 3, 2012]: Thank you, Laura – that’s a good point. I think it is useful to emphasize that RDF and RDFS are both part of a common “language”, but specifically acknowledging that there the language encompasses multiple vocabularies — one might want to include OWL as well — is a good idea.
- [Jason Zou on June 28, 2012]: A further reading section may be necessary for students who want to know more.
- [Jane Greenberg on July 17, 2012]: This is an excellent section to include. 3 quick suggestions: Perhaps make pointed to DCMI glossary as a complementary resources, also, consider adding LOD and LOV.

